{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import keras\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "from cleverhans.utils_mnist import data_mnist\n",
    "from cleverhans.utils import batch_indices\n",
    "from cleverhans.attacks import DeepFool\n",
    "from cleverhans.utils_keras import KerasModelWrapper\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Kojin's comment\n",
    "indices.npy has indices that were kept (among the 784)\n",
    "10% were dropped\n",
    "each log reg was trained for 100 epochs. See my code for details.\n",
    "you can load .npy files with np.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_NOISES = 10\n",
    "NUM_PIXELS = 784\n",
    "NUM_IMAGES = 60000 # number of original (uncorrupted) training points\n",
    "NUM_CLASSES = 10 # 10 classes, 0 to 9 for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/MNIST\", one_hot=True)\n",
    "# size (60000, 10) = (NUM_IMAGES, NUM_CLASSES) -- Kojin's data includes both 'train' and 'validation'\n",
    "# each row is a one-hot encoding of the 10 classes\n",
    "images = np.vstack((mnist.train.images, mnist.validation.images))\n",
    "labels = np.vstack((mnist.train.labels, mnist.validation.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# advx is of length 392, the changed part of the mnist data\n",
    "# we need to recreate the full perturbed data for robust training\n",
    "# we call these advf (adversarial full)\n",
    "\n",
    "\n",
    "# advf_all = np.zeros((NUM_IMAGES, NUM_PIXELS, NUM_NOISES))\n",
    "# indices_all = np.zeros((NUM_PIXELS, NUM_NOISES))\n",
    "\n",
    "for i in range(NUM_NOISES):\n",
    "    advx = np.load(\"../../data/advx_{}.npy\".format(i))\n",
    "    ind  = np.load(\"../../data/indices_{}.npy\".format(i))\n",
    "    advf = copy.deepcopy(images)\n",
    "    advf[:,ind] = advx\n",
    "    advf.dump(\"../../data/advf_{}.npy\".format(i))\n",
    "    # advx_all[:,:,i]  = np.load(\"../../data/advx_{}.npy\".format(i))\n",
    "    # indices_all[:,i] = np.load(\"../../data/indices_{}.npy\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "advf_all = np.zeros((NUM_IMAGES, NUM_PIXELS, NUM_NOISES))\n",
    "for i in range(NUM_NOISES):\n",
    "    advf_all[:,:,i]  = np.load(\"../../data/advf_{}.npy\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from logistic_noise_generator.ipynb, I didn't use this\n",
    "def logistic_regression_model(input_ph,num_inputs, nb_classes=10):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(nb_classes,input_shape=(num_inputs,)))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I implement the Composite Method here\n",
    "\n",
    "NUM_ORACLE_ITER = 2 # Big T in the paper\n",
    "MINIBATCH_SIZE = 100  # don't really need minibatch in logistic regression\n",
    "NUM_TRAINING_ITER = 100 # how many iterations to do the network training like SGD within the oracle \n",
    "\n",
    "weights_distribution = np.full((NUM_ORACLE_ITER, NUM_NOISES), 1./NUM_NOISES) # each row is w_t, simplex vector over noises\n",
    "losses = np.zeros((NUM_ORACLE_ITER, NUM_NOISES)) # has value L_i(x_t) for each i in noises and t in oracle_iter\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "\n",
    "# number of times the Bayesian oracle is invoked\n",
    "for oracle_iter in range(NUM_ORACLE_ITER):\n",
    "    # compute the weights for the distributional oracle for this iteration\n",
    "    eta = np.sqrt(np.log(NUM_NOISES)/(2*NUM_ORACLE_ITER))\n",
    "    unnormalized_current_weights = np.exp(eta*losses[0:oracle_iter,:].sum(axis=0))\n",
    "    weights_distribution[oracle_iter,:] = unnormalized_current_weights/np.sum(unnormalized_current_weights)\n",
    "    # before creating a tensorflow session, first create a computational graph\n",
    "    \n",
    "\n",
    "    # the oracle is logistic regression\n",
    "    train_data   = tf.placeholder(tf.float32, shape=(MINIBATCH_SIZE, NUM_PIXELS, NUM_NOISES))\n",
    "    train_labels = tf.placeholder(tf.float32, shape=(MINIBATCH_SIZE, NUM_CLASSES))\n",
    "    logit_weight = tf.Variable(tf.zeros([NUM_PIXELS, NUM_CLASSES]))\n",
    "    logit_bias   = tf.Variable(tf.zeros([NUM_CLASSES]))\n",
    "    combined_loss = 0\n",
    "    # loss_total_all_noises = tf.Variable(tf.zeros([NUM_NOISES]))\n",
    "    loss_total_list = []\n",
    "    \n",
    "    # in theory, this additional for-loop can be implemented as an extra layer on NN\n",
    "    # but the logit_weight and logit_bias must be shared and I'm not sure how\n",
    "    # I don't think this is any slower, but it's definitely clearer\n",
    "    for noise_type in range(NUM_NOISES):\n",
    "        # simple linear (the logistic part is in the cross_entropy_with_logits)\n",
    "        unscaled_pred = (tf.matmul(train_data[:,:,noise_type], logit_weight)\n",
    "                             + logit_bias)\n",
    "        # loss_vector_this_noise is a vector of length MINIBATCH_SIZE\n",
    "        loss_vector_this_noise = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                        labels=train_labels,\n",
    "                        logits=unscaled_pred,\n",
    "                        )\n",
    "        loss_total_this_noise = tf.reduce_sum(loss_vector_this_noise)\n",
    "        loss_total_list.append(loss_total_this_noise)\n",
    "        combined_loss += weights_distribution[oracle_iter,noise_type]*loss_total_this_noise\n",
    "    loss_total_all_noises = tf.stack(loss_total_list)\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(combined_loss)\n",
    "    \n",
    "    # now that the computational graph is finished, start doing the computation\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for training_iter in range(NUM_TRAINING_ITER):\n",
    "            minibatch_subset = np.random.choice(np.arange(NUM_IMAGES), size=MINIBATCH_SIZE, replace=False)\n",
    "            minibatch_data = advf_all[minibatch_subset,:,:]\n",
    "            minibatch_labels = labels[minibatch_subset,:]\n",
    "            train_step.run(feed_dict={train_data:minibatch_data,train_labels:minibatch_labels})\n",
    "        # training is done\n",
    "        # keep the loss for this oracle iteration in 'losses' \n",
    "        losses[oracle_iter,:] = loss_total_all_noises.eval(\n",
    "            feed_dict={train_data:minibatch_data,train_labels:minibatch_labels})\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.4'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.1'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 1s 964us/step - loss: 2.4086 - acc: 0.0890\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 2.3713 - acc: 0.1080\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 2.3362 - acc: 0.1190\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 2.3208 - acc: 0.1190\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 2.3190 - acc: 0.1180\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 2.3084 - acc: 0.1110\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 2.3134 - acc: 0.0930\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 2.3009 - acc: 0.1170\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 2.3033 - acc: 0.1190\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 2.3006 - acc: 0.1210\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 2.3035 - acc: 0.1090\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 0s 56us/step - loss: 2.2941 - acc: 0.1160\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 2.3058 - acc: 0.1130\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 2.2976 - acc: 0.1110\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 0s 127us/step - loss: 2.2956 - acc: 0.1280\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 2.2961 - acc: 0.1380\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 2.2917 - acc: 0.1380\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 2.2946 - acc: 0.1210\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 2.2929 - acc: 0.1110\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 2.2966 - acc: 0.1230\n",
      "100/100 [==============================] - 0s 499us/step\n"
     ]
    }
   ],
   "source": [
    "# ignore this one, this is just copied from a keras tutorial\n",
    "# I didn't use keras in my code\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "x_train = np.random.random((1000, 20))\n",
    "y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10)\n",
    "x_test = np.random.random((100, 20))\n",
    "y_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n",
    "\n",
    "model = Sequential()\n",
    "# Dense(64) is a fully-connected layer with 64 hidden units.\n",
    "# in the first layer, you must specify the expected input data shape:\n",
    "# here, 20-dimensional vectors.\n",
    "model.add(Dense(64, activation='relu', input_dim=20))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128)\n",
    "score = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.full((3,3),9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.array([[1,2,3], [4,5,6],[7,8,9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [7, 8, 9]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[[0,2],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "[2 1 3]\n",
      "[3 1 3]\n",
      "[3 3 2]\n",
      "[3 1 1]\n",
      "[3 0 1]\n",
      "[2 3 1]\n",
      "[3 0 3]\n",
      "[0 0 3]\n",
      "[0 0 0]\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(np.random.choice(np.arange(4), size=3, replace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
