{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from cleverhans.utils_mnist import data_mnist\n",
    "from cleverhans.utils import batch_indices\n",
    "from cleverhans.attacks import DeepFool\n",
    "from cleverhans.utils_keras import KerasModelWrapper\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "epochs = 10\n",
    "batch_size = 128 \n",
    "drop_ratio = 0.10\n",
    "num_inputs = int(28 * 28 * (1-drop_ratio))\n",
    "noise_eps = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_model(input_ph,num_inputs, nb_classes=10):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(nb_classes,input_shape=(num_inputs,)))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExportMNISTGraph():\n",
    "    def __init__(self,seed=0):\n",
    "        np.random.seed(seed)\n",
    "        # Create local graph and use it in the self.session\n",
    "        self.graph = tf.Graph()\n",
    "        self.sess = tf.Session(graph=self.graph)\n",
    "        X_train, Y_train, X_test, Y_test = data_mnist()\n",
    "        # Flatten image matrices\n",
    "        num_train, num_rows, num_cols, num_channels = X_train.shape\n",
    "        num_test, _, _, _ = X_test.shape\n",
    "        X_train = X_train.reshape((num_train, num_rows * num_cols * num_channels))\n",
    "        X_test = X_test.reshape((num_test, num_rows * num_cols * num_channels))\n",
    "        # Randomly drop some ratio of pixels\n",
    "        input_indices = np.random.choice(\n",
    "            num_rows * num_cols, num_inputs, replace=False)\n",
    "        self.X_train = X_train[:, input_indices]\n",
    "        self.X_test = X_test[:, input_indices]\n",
    "        \n",
    "        self.Y_train = Y_train\n",
    "        self.Y_test = Y_test\n",
    "        \n",
    "    \n",
    "    def train(self,model_name):\n",
    "        with self.graph.as_default():\n",
    "            # Define placeholders\n",
    "            x = tf.placeholder(tf.float32, shape=(None, num_inputs))\n",
    "            y = tf.placeholder(tf.float32, shape=(None, 10))\n",
    "\n",
    "            # Wrap log reg model for applying adversarial examples\n",
    "            model = logistic_regression_model(x,num_inputs)\n",
    "            tf.add_to_collection(model_name, model)\n",
    "\n",
    "            wrap = KerasModelWrapper(model, 10) \n",
    "\n",
    "            # Define the objective\n",
    "            logits = wrap.get_logits(x)\n",
    "            loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y,logits=logits)\n",
    "\n",
    "            # Define the update func\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "            train_step = optimizer.minimize(loss)\n",
    "            acc, acc_op = tf.metrics.accuracy(labels=tf.argmax(y, 1), \n",
    "                                              predictions=tf.argmax(logits,1))\n",
    "\n",
    "            # Define adv attack\n",
    "            deepfool = DeepFool(wrap, sess=self.sess)\n",
    "            deepfool_params = {'eps': noise_eps,\n",
    "                           'clip_min': 0.,\n",
    "                           'clip_max': 1.}\n",
    "\n",
    "            # Attack images\n",
    "            adv_x = deepfool.generate(x, **deepfool_params)\n",
    "            # Consider the attack to be constant\n",
    "#             adv_x = tf.stop_gradient(adv_x)\n",
    "            tf.add_to_collection(\"adv\", adv_x)\n",
    "\n",
    "            # Evaluate predictions on adv attacks\n",
    "            preds = model(adv_x)\n",
    "            acc_adv, acc_op_adv = tf.metrics.accuracy(labels=tf.argmax(y, 1), \n",
    "                                              predictions=tf.argmax(preds,1))\n",
    "\n",
    "            # Initialize variables\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "            self.sess.run(tf.local_variables_initializer())\n",
    "            saver = tf.train.Saver({'advx_x':adv_x})\n",
    "            rng = np.random.RandomState() # for batch sampling\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                # Compute number of batches\n",
    "                num_batches = int(math.ceil(float(len(self.X_train)) / batch_size))\n",
    "                assert num_batches * batch_size >= len(self.X_train)\n",
    "\n",
    "                # Indices to shuffle training set\n",
    "                index_shuf = list(range(len(self.X_train)))\n",
    "                rng.shuffle(index_shuf)\n",
    "                for batch in range(num_batches):\n",
    "\n",
    "                    # Compute batch start and end indices\n",
    "                    start, end = batch_indices(\n",
    "                        batch, len(self.X_train), batch_size)\n",
    "\n",
    "                    # Perform one training step\n",
    "                    feed_dict = {x: self.X_train[index_shuf[start:end]],\n",
    "                                 y: self.Y_train[index_shuf[start:end]]}\n",
    "                    self.sess.run(train_step,feed_dict=feed_dict)\n",
    "\n",
    "                feed_dict = {x: self.X_train,\n",
    "                             y: self.Y_train}\n",
    "                acc_val = self.sess.run(acc_op,feed_dict=feed_dict)\n",
    "                print(f\"Epoch: {epoch}, Train Acc: {acc_val:.5f}\")\n",
    "#             # Evaluate the model on the test set\n",
    "#             feed_dict = {x: self.X_test, y: self.Y_test}\n",
    "#             test_acc = self.sess.run(acc_op, feed_dict=feed_dict)\n",
    "#             print(f\"Test Acc: {test_acc:.5f}\")\n",
    "\n",
    "#             # Evaluate the model on the adversarial test set\n",
    "#             test_acc_adv = self.sess.run(acc_op_adv, feed_dict=feed_dict)\n",
    "#             print(f\"Test Acc Adv: {test_acc_adv:.5f}\")\n",
    "\n",
    "            model_path = f\"../models/{model_name}\"\n",
    "            os.makedirs(model_path, exist_ok=True)\n",
    "            saver.save(self.sess, f\"{model_path}/{model_name}\")\n",
    "            print(f\"Model saved to {model_path}/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "names_to_saveables must be a dict mapping string names to Tensors/Variables. Not a variable: Tensor(\"PyFunc:0\", dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-1c4df08d3206>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExportMNISTGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'model{i}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-8ed606aac9cb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model_name)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'advx_x'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0madv_x\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0mrng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# for batch sampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, var_list, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, saver_def, builder, defer_build, allow_empty, write_version, pad_step_number, save_relative_paths, filename)\u001b[0m\n\u001b[1;32m   1336\u001b[0m           time.time() + self._keep_checkpoint_every_n_hours * 3600)\n\u001b[1;32m   1337\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdefer_build\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1338\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1339\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_saver_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1345\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Use save/restore instead of build in eager mode.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1347\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_build_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m_build\u001b[0;34m(self, checkpoint_path, build_save, build_restore)\u001b[0m\n\u001b[1;32m   1382\u001b[0m           \u001b[0mrestore_sequentially\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore_sequentially\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m           \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m           build_save=build_save, build_restore=build_restore)\n\u001b[0m\u001b[1;32m   1385\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m       \u001b[0;31m# Since self._name is used as a name_scope by builder(), we are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m_build_internal\u001b[0;34m(self, names_to_saveables, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, filename, build_save, build_restore)\u001b[0m\n\u001b[1;32m    811\u001b[0m                        \" when eager execution is not enabled.\")\n\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 813\u001b[0;31m     \u001b[0msaveables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ValidateAndSliceInputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames_to_saveables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    814\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmax_to_keep\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m       \u001b[0mmax_to_keep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m_ValidateAndSliceInputs\u001b[0;34m(self, names_to_saveables)\u001b[0m\n\u001b[1;32m    712\u001b[0m             raise TypeError(\"names_to_saveables must be a dict mapping string \"\n\u001b[1;32m    713\u001b[0m                             \u001b[0;34m\"names to Tensors/Variables. Not a variable: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m                             variable)\n\u001b[0m\u001b[1;32m    715\u001b[0m           if variable.op.type in [\"Variable\", \"VariableV2\",\n\u001b[1;32m    716\u001b[0m                                   \"AutoReloadVariable\"]:\n",
      "\u001b[0;31mTypeError\u001b[0m: names_to_saveables must be a dict mapping string names to Tensors/Variables. Not a variable: Tensor(\"PyFunc:0\", dtype=float32)"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    graph = ExportMNISTGraph(i)\n",
    "    graph.train(f'model{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('__variable_store',): [<tensorflow.python.ops.variable_scope._VariableStore at 0x1218d3ac8>],\n",
       " ('__varscope',): [<tensorflow.python.ops.variable_scope._VariableScopeStore at 0x1218c1a68>],\n",
       " 'adv': [<tf.Tensor 'StopGradient:0' shape=<unknown> dtype=float32>],\n",
       " 'local_variables': [<tf.Variable 'accuracy/total:0' shape=() dtype=float32_ref>,\n",
       "  <tf.Variable 'accuracy/count:0' shape=() dtype=float32_ref>,\n",
       "  <tf.Variable 'accuracy_1/total:0' shape=() dtype=float32_ref>,\n",
       "  <tf.Variable 'accuracy_1/count:0' shape=() dtype=float32_ref>],\n",
       " 'metric_variables': [<tf.Variable 'accuracy/total:0' shape=() dtype=float32_ref>,\n",
       "  <tf.Variable 'accuracy/count:0' shape=() dtype=float32_ref>,\n",
       "  <tf.Variable 'accuracy_1/total:0' shape=() dtype=float32_ref>,\n",
       "  <tf.Variable 'accuracy_1/count:0' shape=() dtype=float32_ref>],\n",
       " 'model2': [<keras.models.Sequential at 0x121141eb8>],\n",
       " 'train_op': [<tf.Operation 'Adam' type=NoOp>],\n",
       " 'trainable_variables': [<tf.Variable 'dense_1/kernel:0' shape=(705, 10) dtype=float32_ref>,\n",
       "  <tf.Variable 'dense_1/bias:0' shape=(10,) dtype=float32_ref>],\n",
       " 'variables': [<tf.Variable 'dense_1/kernel:0' shape=(705, 10) dtype=float32_ref>,\n",
       "  <tf.Variable 'dense_1/bias:0' shape=(10,) dtype=float32_ref>,\n",
       "  <tf.Variable 'beta1_power:0' shape=() dtype=float32_ref>,\n",
       "  <tf.Variable 'beta2_power:0' shape=() dtype=float32_ref>,\n",
       "  <tf.Variable 'dense_1/kernel/Adam:0' shape=(705, 10) dtype=float32_ref>,\n",
       "  <tf.Variable 'dense_1/kernel/Adam_1:0' shape=(705, 10) dtype=float32_ref>,\n",
       "  <tf.Variable 'dense_1/bias/Adam:0' shape=(10,) dtype=float32_ref>,\n",
       "  <tf.Variable 'dense_1/bias/Adam_1:0' shape=(10,) dtype=float32_ref>]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.graph._collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable_variables\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'coll' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-ccec7e921f8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collections\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'coll' is not defined"
     ]
    }
   ],
   "source": [
    "for name in graph.graph._collections:\n",
    "    print(name)\n",
    "    print('%s: %s' % (name, set(map(type, coll))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImportGraph():\n",
    "    \"\"\"  Importing and running isolated TF graph \"\"\"\n",
    "    def __init__(self, loc):\n",
    "        # Create local graph and use it in the session\n",
    "        self.graph = tf.Graph()\n",
    "        self.sess = tf.Session(graph=self.graph)\n",
    "        with self.graph.as_default():\n",
    "            # Import saved model from location 'loc' into local graph\n",
    "            saver = tf.train.import_meta_graph(loc + '.meta',\n",
    "                                               clear_devices=True)\n",
    "            saver.restore(self.sess, loc)\n",
    "            # There are TWO options how to get activation operation:\n",
    "              # FROM SAVED COLLECTION:            \n",
    "            self.activation = tf.get_collection('activation')[0]\n",
    "              # BY NAME:\n",
    "            self.activation = self.graph.get_operation_by_name('activation_opt').outputs[0]\n",
    "\n",
    "    def run(self, data):\n",
    "        \"\"\" Running the activation operation previously imported \"\"\"\n",
    "        # The 'x' corresponds to name of input placeholder\n",
    "        return self.sess.run(self.activation, feed_dict={\"x:0\": data})\n",
    "      \n",
    "      \n",
    "    ### Using the class ###\n",
    "    data = 50         # random data\n",
    "    model = ImportGraph('models/model_name')\n",
    "    result = model.run(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
