{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from cleverhans.attacks import DeepFool, MomentumIterativeMethod\n",
    "from cleverhans.utils import batch_indices\n",
    "from cleverhans.utils_keras import KerasModelWrapper\n",
    "from utils import (fully_connected_nn_model, load_data, to_onehot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_layers=2\n",
    "noise_eps=0.1\n",
    "lr=0.001\n",
    "epochs=2\n",
    "batch_size=128\n",
    "num_noises=3\n",
    "num_oracle_iter=2\n",
    "data='mnist'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data\n",
    "X_train, Y_train, X_test, Y_test = load_data(data)\n",
    "num_train, num_rows, num_cols, num_channels = X_train.shape\n",
    "_, num_classes = Y_train.shape\n",
    "\n",
    "# Load adversarial examples generated from logistic regressions\n",
    "X_train_adv = np.zeros((num_noises, num_train, num_rows, num_cols,\n",
    "                        num_channels))\n",
    "X_train_adv[0] = X_train\n",
    "for i in range(num_noises):\n",
    "    X_train_adv[i] = np.load(f\"../data/{data}/logreg_adv/adv_{i}.npy\")\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "x = tf.placeholder(\n",
    "    tf.float32,\n",
    "    shape=(num_noises, None, num_rows, num_cols, num_channels))\n",
    "y = tf.placeholder(tf.float32, shape=(None, num_classes))\n",
    "w = tf.placeholder(tf.float32, shape=(num_noises, ))\n",
    "\n",
    "model = fully_connected_nn_model(\n",
    "    (num_rows, num_cols, num_channels),\n",
    "    num_hidden_layers=num_hidden_layers)\n",
    "wrap = KerasModelWrapper(model, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = tf.zeros([0], tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = tf.zeros([0], tf.float32)\n",
    "\n",
    "def body(i, losses):\n",
    "    logits = wrap.get_logits(x[i])\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=logits))\n",
    "    losses = tf.concat([losses, [loss]], 0)\n",
    "    return i + 1, losses\n",
    "\n",
    "\n",
    "def condition(i, losses):\n",
    "    return i < num_noises\n",
    "\n",
    "\n",
    "_, losses = tf.while_loop(condition, body, [0, losses], \n",
    "                        shape_invariants=[tf.TensorShape(None),tf.TensorShape([None])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = w * losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the update func\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "train_step = optimizer.minimize(loss)\n",
    "\n",
    "logits = wrap.get_logits(x[0])\n",
    "acc, acc_op = tf.metrics.accuracy(\n",
    "    labels=tf.argmax(y, 1), predictions=tf.argmax(logits, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/kojin/projects/robust-opt-adversarial/cleverhans/cleverhans/attacks.py:563: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/22/2018 11:17:36 PM: From /Users/kojin/projects/robust-opt-adversarial/cleverhans/cleverhans/attacks.py:563: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/kojin/projects/robust-opt-adversarial/cleverhans/cleverhans/attacks.py:585: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/22/2018 11:17:37 PM: From /Users/kojin/projects/robust-opt-adversarial/cleverhans/cleverhans/attacks.py:585: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "# Define adv attack\n",
    "deepfool = DeepFool(wrap, sess=sess)\n",
    "deepfool_params = {'eps': noise_eps, 'clip_min': 0., 'clip_max': 1.}\n",
    "\n",
    "# Attack images\n",
    "x_deepfool = deepfool.generate(x[0], **deepfool_params)\n",
    "# Consider the attack to be constant\n",
    "x_deepfool = tf.stop_gradient(x_deepfool)\n",
    "\n",
    "# Evaluate predictions on adv attacks\n",
    "preds_deepfool = model(x_deepfool)\n",
    "acc_deepfool, acc_op_deepfool = tf.metrics.accuracy(\n",
    "    labels=tf.argmax(y, 1), predictions=tf.argmax(preds_deepfool, 1))\n",
    "\n",
    "# Define adv attack\n",
    "momentum_iterative = MomentumIterativeMethod(wrap, sess=sess)\n",
    "momentum_iterative_params = {\n",
    "    'eps': noise_eps,\n",
    "    'clip_min': 0.,\n",
    "    'clip_max': 1.\n",
    "}\n",
    "\n",
    "# Attack images\n",
    "x_momentum_iterative = momentum_iterative.generate(x[0], **deepfool_params)\n",
    "# Consider the attack to be constant\n",
    "x_momentum_iterative = tf.stop_gradient(x_momentum_iterative)\n",
    "\n",
    "# Evaluate predictions on adv attacks\n",
    "preds_momentum_iterative = model(x_momentum_iterative)\n",
    "acc_momentum_iterative, acc_op_momentum_iterative = tf.metrics.accuracy(\n",
    "    labels=tf.argmax(y, 1), predictions=tf.argmax(preds_momentum_iterative, 1))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "rng = np.random.RandomState()  # for batch sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/22/2018 11:19:30 PM: oracle_iter: 0\n",
      "07/22/2018 11:19:30 PM: Noise ratio: f[ 0.33333333  0.33333333  0.33333333]\n",
      "07/22/2018 11:19:30 PM: epoch: 0\n",
      "07/22/2018 11:20:17 PM: epoch: 1\n",
      "07/22/2018 11:20:50 PM: oracle_iter: 1\n",
      "07/22/2018 11:20:50 PM: Noise ratio: f[ 0.33341153  0.33317991  0.33340857]\n",
      "07/22/2018 11:20:50 PM: epoch: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-7bbb984cb102>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mweights_distribution\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moracle_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             }\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mloss_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0;31m# Normalize and log loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mloss_total_list\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_vals\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 908\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    909\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1141\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1143\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1144\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1324\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1325\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1328\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1330\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1331\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1313\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1315\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1421\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1422\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1423\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Each row is w_t, simplex vector over noises\n",
    "weights_distribution = np.full((num_oracle_iter, num_noises),\n",
    "                               1. / num_noises)\n",
    "# Has value L_i(x_t) for each i in noises and t in oracle_iter\n",
    "losses_np = np.zeros((num_oracle_iter, num_noises))\n",
    "\n",
    "# Number of times the Bayesian oracle is invoked\n",
    "for oracle_iter in range(num_oracle_iter):\n",
    "    logging.info(f'oracle_iter: {oracle_iter}')\n",
    "    # initialize variables every iteration\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "\n",
    "    ### Compute the weights for the distributional oracle for this iteration ###\n",
    "    # eta is the time dependent parameter controlling the weight distribution\n",
    "    eta = np.sqrt(np.log(num_noises) / (2 * num_oracle_iter))\n",
    "    # See Algorithm 1 (3) for this udpate\n",
    "    unnormalized_current_weights = np.exp(\n",
    "        eta * losses_np[0:oracle_iter, :].sum(axis=0))\n",
    "    normalized_current_weights = unnormalized_current_weights / np.sum(\n",
    "        unnormalized_current_weights)\n",
    "    # Log current weights for later iterations\n",
    "    weights_distribution[oracle_iter, :] = normalized_current_weights\n",
    "    logging.info(f\"Noise ratio: f{normalized_current_weights}\")\n",
    "\n",
    "    ### Train model with weighted loss for each noise ###\n",
    "    loss_total_list = np.zeros((num_noises, ))\n",
    "    for epoch in range(epochs):\n",
    "        logging.info(f'epoch: {epoch}')\n",
    "        # Compute number of batches\n",
    "        num_batches = int(math.ceil(num_train / batch_size))\n",
    "        assert num_batches * batch_size >= num_train\n",
    "\n",
    "        # Indices to shuffle training set\n",
    "        index_shuf = list(range(num_train))\n",
    "        rng.shuffle(index_shuf)\n",
    "        for batch in range(num_batches):\n",
    "            # Compute batch start and end indices\n",
    "            start, end = batch_indices(batch, num_train, batch_size)\n",
    "\n",
    "            feed_dict = {\n",
    "                x: X_train_adv[:, index_shuf[start:end]],\n",
    "                y: Y_train[index_shuf[start:end]],\n",
    "                w: weights_distribution[oracle_iter]\n",
    "            }\n",
    "            loss_vals, _ = sess.run([losses, train_step], feed_dict=feed_dict)\n",
    "            # Normalize and log loss\n",
    "            loss_total_list += loss_vals / (epochs * num_batches)\n",
    "    # Save average loss for the next iteration\n",
    "    losses_np[oracle_iter, :] = loss_total_list\n",
    "\n",
    "# Save the last trained model\n",
    "saver.save(sess, \"../model/robust_optimization/model.ckpt\")\n",
    "\n",
    "# 1. Accuracy on uncorrupted test set\n",
    "feed_dict = {x: X_test, y: Y_test}\n",
    "test_acc = sess.run(acc_op, feed_dict=feed_dict)\n",
    "logger.info(f\"Test Acc: {test_acc}\")\n",
    "\n",
    "# 2. Accuracy on test set corrupted by known noises that are used during training\n",
    "test_acc_deepfool = sess.run(acc_op_deepfool, feed_dict=feed_dict)\n",
    "logger.info(f\"Test Acc Deepfool: {test_acc_deepfool}\")\n",
    "\n",
    "# 3. Accuracy on test set corrupted by adversarial noises not used during training\n",
    "test_acc_momentum_iterative = sess.run(\n",
    "    acc_op_momentum_iterative, feed_dict=feed_dict)\n",
    "logger.info(f\"Test Acc Momentum Iterative: {test_acc_momentum_iterative}\")\n",
    "\n",
    "# Log experiment result\n",
    "path = f'../result/robust_optimization'\n",
    "os.makedirs(path, exist_ok=True)\n",
    "losses.dump(f'{path}/losses.npy')\n",
    "weights_distribution.dump(f'{path}/weights_distribution.npy')\n",
    "accs = np.array([test_acc, test_acc_deepfool, test_acc_momentum_iterative])\n",
    "accs.dump(f'{path}/accs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
